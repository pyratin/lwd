{"version":3,"sources":["../../../../../js/server/schema/mutations/fns/wordsTokenizedGet.js"],"names":["_sentence","sentence","replace","tokenizer","natural","TreebankWordTokenizer","words","tokenize","reduce","memo","word","index","text"],"mappings":"AAAA;;;;;;;;;;;AAEA;;eAEe,kBACbA,SADa,EAEV;AAEH,MAAMC,QAAQ,GAAGD,SAAS,CACvBE,OADc,CAEb,KAFa,EAGb,KAHa,CAAjB;;AAMA,MAAMC,SAAS,GAAG,IAAIC,oBAAQC,qBAAZ,EAAlB;AAEA,MAAMC,KAAK,GAAGH,SAAS,CAACI,QAAV,CACZN,QADY,EAGXO,MAHW,CAIV,UACEC,IADF,EAEEC,IAFF,EAGEC,KAHF,EAIK;AAEH,yDACKF,IADL,IAEE;AACEG,MAAAA,IAAI,EAAEF,IADR;AAEEC,MAAAA,KAAK,EAALA;AAFF,KAFF;AAOD,GAjBS,EAkBV,EAlBU,CAAd;AAqBA,SACEL,KADF;AAGD,C","sourcesContent":["'use strict';\n\nimport natural from 'natural';\n\nexport default (\n  _sentence\n) => {\n\n  const sentence = _sentence\n    .replace(\n      /\\//g,\n      ' / '\n    );\n\n  const tokenizer = new natural.TreebankWordTokenizer();\n\n  const words = tokenizer.tokenize(\n    sentence\n  )\n    .reduce(\n      (\n        memo,\n        word,\n        index\n      ) => {\n\n        return [\n          ...memo,\n          {\n            text: word,\n            index\n          }\n        ];\n      },\n      []\n    );\n\n  return (\n    words\n  );\n};\n\n"],"file":"wordsTokenizedGet.js"}