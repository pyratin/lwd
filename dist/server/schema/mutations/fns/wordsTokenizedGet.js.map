{"version":3,"sources":["../../../../../js/server/schema/mutations/fns/wordsTokenizedGet.js"],"names":["_sentence","sentence","replace","tokenizer","natural","TreebankWordTokenizer","words","tokenize","reduce","memo","word","index","text","regExpString","regExpStringMemo","trim","regExp","RegExp","match","distance","length"],"mappings":"AAAA;;;;;;;;;;;;;AAEA;;AACA;;;;;;eAEe,kBACbA,SADa,EAEV;AAEH,MAAMC,QAAQ,GAAGD,SAAS,CACvBE,OADc,CAEb,aAFa,EAGb,SAHa,CAAjB;;AAMA,MAAMC,SAAS,GAAG,IAAIC,oBAAQC,qBAAZ,EAAlB;AAEA,MAAIC,KAAK,GAAGH,SAAS,CAACI,QAAV,CACVN,QADU,EAGTO,MAHS,CAIR,UACEC,IADF,EAEEC,IAFF,EAGEC,KAHF,EAIK;AAEH,yDACKF,IADL,IAEE;AACEG,MAAAA,IAAI,EAAEF,IADR;AAEEC,MAAAA,KAAK,EAALA;AAFF,KAFF;AAOD,GAjBO,EAkBR,EAlBQ,CAAZ;AAqBAL,EAAAA,KAAK,GAAGA,KAAK,CAACE,MAAN,CACN,UACEC,IADF,EAEEC,IAFF,EAGK;AAEH,QAAIG,YAAY,GAAGJ,IAAI,CAACD,MAAL,CACjB,UACEM,gBADF,QAKK;AAAA,UAFDF,IAEC,QAFDA,IAEC;AAEH,aAAO,wBAEHE,gBAFG,iBAIH,oCACEF,IADF,CAJG,kBASJG,IATI,EAAP;AAUD,KAlBgB,EAmBjB,EAnBiB,CAAnB;AAsBAF,IAAAA,YAAY,GAAG,qBAEXA,YAFW,kBAKZE,IALY,EAAf;AAOA,QAAMC,MAAM,GAAG,IAAIC,MAAJ,CACbJ,YADa,CAAf;;AAIA,QAAMK,KAAK,GAAGlB,SAAS,CAACkB,KAAV,CACZF,MADY,CAAd;;AAIA,yDACKP,IADL,oCAGOC,IAHP;AAIIS,MAAAA,QAAQ,EAAED,KAAF,aAAEA,KAAF,uBAAEA,KAAK,CACb,CADa,CAAL,CAGPE;AAPP;AAUD,GArDK,EAsDN,EAtDM,CAAR;AAyDA,SACEd,KADF;AAGD,C","sourcesContent":["'use strict';\n\nimport natural from 'natural';\nimport escapeStringRegexp from 'escape-string-regexp';\n\nexport default (\n  _sentence\n) => {\n\n  const sentence = _sentence\n    .replace(\n      /(\\S)\\/(\\S)/g,\n      '$1 / $2'\n    );\n\n  const tokenizer = new natural.TreebankWordTokenizer();\n\n  let words = tokenizer.tokenize(\n    sentence\n  )\n    .reduce(\n      (\n        memo,\n        word,\n        index\n      ) => {\n\n        return [\n          ...memo,\n          {\n            text: word,\n            index\n          }\n        ];\n      },\n      []\n    );\n\n  words = words.reduce(\n    (\n      memo,\n      word\n    ) => {\n\n      let regExpString = memo.reduce(\n        (\n          regExpStringMemo,\n          {\n            text\n          }\n        ) => {\n\n          return `\n            ${\n              regExpStringMemo\n            }\\\\s*${\n              escapeStringRegexp(\n                text\n              )\n            }\n          `\n            .trim();\n        },\n        ''\n      );\n\n      regExpString = `\n        ^${\n          regExpString\n        }\\\\s*\n      `\n        .trim();\n\n      const regExp = new RegExp(\n        regExpString\n      );\n\n      const match = _sentence.match(\n        regExp\n      );\n\n      return [\n        ...memo,\n        {\n          ...word,\n          distance: match?.[\n            0\n          ]\n            .length\n        }\n      ];\n    },\n    []\n  );\n\n  return (\n    words\n  );\n};\n\n"],"file":"wordsTokenizedGet.js"}