{"version":3,"sources":["../../../../../js/server/schema/mutations/fns/wordsTokenizedGet.js"],"names":["tokenizerGet","tokenizerType","natural","WordPunctTokenizer","TreebankWordTokenizer","_sentence","tokenizer","sentence","replace","words","tokenize","reduce","memo","word","tokenIndex","text","regExpString","regExpStringMemo","trim","regExp","RegExp","match","distance","length"],"mappings":"AAAA;;;;;;;;;;;;;AAEA;;AACA;;;;;;AAEA,IAAMA,YAAY,GAAG,SAAfA,YAAe,CACnBC,aADmB,EAEhB;AAEH,UACEA,aADF;AAIE,SACE,WADF;AAIE,aAAO,IAAIC,oBAAQC,kBAAZ,EAAP;;AAEF;AAEE,aAAO,IAAID,oBAAQE,qBAAZ,EAAP;AAZJ;AAcD,CAlBD;;eAoBe,kBACbC,SADa,EAGV;AAAA,MADHJ,aACG,uEADa,UACb;AAEH,MAAMK,SAAS,GAAGN,YAAY,CAC5BC,aAD4B,CAA9B;;AAIA,MAAMM,QAAQ,GAAGF,SAAS,CACvBG,OADc,CAEb,aAFa,EAGb,SAHa,CAAjB;;AAMA,MAAIC,KAAK,GAAGH,SAAS,CAACI,QAAV,CACVH,QADU,EAGTI,MAHS,CAIR,UACEC,IADF,EAEEC,IAFF,EAGEC,UAHF,EAIK;AAEH,yDACKF,IADL,IAEE;AACEG,MAAAA,IAAI,EAAEF,IADR;AAEEC,MAAAA,UAAU,EAAVA;AAFF,KAFF;AAOD,GAjBO,EAkBR,EAlBQ,CAAZ;AAqBAL,EAAAA,KAAK,GAAGA,KAAK,CAACE,MAAN,CACN,UACEC,IADF,EAEEC,IAFF,EAGK;AAEH,QAAIG,YAAY,GAAGJ,IAAI,CAACD,MAAL,CACjB,UACEM,gBADF,QAKK;AAAA,UAFDF,IAEC,QAFDA,IAEC;AAEH,aAAO,wBAEHE,gBAFG,iBAIH,oCACEF,IADF,CAJG,kBASJG,IATI,EAAP;AAUD,KAlBgB,EAmBjB,EAnBiB,CAAnB;AAsBAF,IAAAA,YAAY,GAAG,qBAEXA,YAFW,kBAKZE,IALY,EAAf;AAOA,QAAMC,MAAM,GAAG,IAAIC,MAAJ,CACbJ,YADa,CAAf;;AAIA,QAAMK,KAAK,GAAGhB,SAAS,CAACgB,KAAV,CACZF,MADY,CAAd;;AAIA,yDACKP,IADL,oCAGOC,IAHP;AAIIS,MAAAA,QAAQ,EAAED,KAAF,aAAEA,KAAF,uBAAEA,KAAK,CACb,CADa,CAAL,CAGPE;AAPP;AAUD,GArDK,EAsDN,EAtDM,CAAR;AAyDA,SACEd,KADF;AAGD,C","sourcesContent":["'use strict';\n\nimport natural from 'natural';\nimport escapeStringRegexp from 'escape-string-regexp';\n\nconst tokenizerGet = (\n  tokenizerType\n) => {\n\n  switch (\n    tokenizerType\n  ) {\n\n    case (\n      'wordPunct'\n    ) :\n\n      return new natural.WordPunctTokenizer();\n\n    default:\n\n      return new natural.TreebankWordTokenizer();\n  }\n};\n\nexport default (\n  _sentence,\n  tokenizerType = 'treebank'\n) => {\n\n  const tokenizer = tokenizerGet(\n    tokenizerType\n  );\n\n  const sentence = _sentence\n    .replace(\n      /(\\S)\\/(\\S)/g,\n      '$1 / $2'\n    );\n\n  let words = tokenizer.tokenize(\n    sentence\n  )\n    .reduce(\n      (\n        memo,\n        word,\n        tokenIndex\n      ) => {\n\n        return [\n          ...memo,\n          {\n            text: word,\n            tokenIndex\n          }\n        ];\n      },\n      []\n    );\n\n  words = words.reduce(\n    (\n      memo,\n      word\n    ) => {\n\n      let regExpString = memo.reduce(\n        (\n          regExpStringMemo,\n          {\n            text\n          }\n        ) => {\n\n          return `\n            ${\n              regExpStringMemo\n            }\\\\s*${\n              escapeStringRegexp(\n                text\n              )\n            }\n          `\n            .trim();\n        },\n        ''\n      );\n\n      regExpString = `\n        ^${\n          regExpString\n        }\\\\s*\n      `\n        .trim();\n\n      const regExp = new RegExp(\n        regExpString\n      );\n\n      const match = _sentence.match(\n        regExp\n      );\n\n      return [\n        ...memo,\n        {\n          ...word,\n          distance: match?.[\n            0\n          ]\n            .length\n        }\n      ];\n    },\n    []\n  );\n\n  return (\n    words\n  );\n};\n\n"],"file":"wordsTokenizedGet.js"}